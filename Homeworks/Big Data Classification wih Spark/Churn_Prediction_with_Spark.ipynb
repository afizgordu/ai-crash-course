{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Churn Prediction with Spark\n",
        "\n",
        "This Jupyter Notebook performs customer churn prediction using Spark. The analysis is conducted with the `churn.csv` dataset, applying classification algorithms to predict the likelihood of customer churn.\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. **Data Loading and Preprocessing:** Loading the `churn.csv` dataset and performing necessary preprocessing steps.\n",
        "2. **Feature Engineering:** Preparing and transforming dataset features for model training.\n",
        "3. **Model Training:** Implementing models such as Gradient Boosted Tree (GBT) classifiers to predict churn.\n",
        "4. **Model Evaluation:** Assessing model performance on test data, including the calculation of metrics like AUC.\n",
        "\n",
        "## Objective\n",
        "\n",
        "The goal of this notebook is to predict customer churn rates and assist in making strategic decisions to reduce customer attrition. The developed model aims to accurately predict the likelihood of customers leaving, potentially helping businesses strengthen their customer relationship strategies.\n"
      ],
      "metadata": {
        "id": "jKf3q57zKwkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install pyspark"
      ],
      "metadata": {
        "id": "B_9mZK2x6oai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, Bucketizer\n",
        "from pyspark.sql.types import IntegerType, FloatType\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ChurnPrediction\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"churn.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Check for missing values\n",
        "df = df.na.drop()\n",
        "df\n",
        "\n",
        "# Handle categorical columns with high cardinality by grouping infrequent categories\n",
        "threshold = 100  # Set a threshold for frequency\n",
        "categorical_cols = [col for col, dtype in df.dtypes if dtype == 'string' and col != 'Churn']\n",
        "\n",
        "def group_infrequent_categories(column, threshold):\n",
        "    counts = df.groupBy(column).count()\n",
        "    frequent_categories = counts.where(F.col('count') >= threshold).select(column).rdd.flatMap(lambda x: x).collect()\n",
        "    return F.when(F.col(column).isin(frequent_categories), F.col(column)).otherwise(\"Other\")\n",
        "\n",
        "# Apply grouping for high cardinality columns\n",
        "for col_name in categorical_cols:\n",
        "    df = df.withColumn(col_name, group_infrequent_categories(col_name, threshold))\n",
        "\n",
        "# Handle categorical columns using StringIndexer\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_Index\") for col in categorical_cols]\n",
        "\n",
        "# Apply the indexers\n",
        "for indexer in indexers:\n",
        "    df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Assemble all features into a single vector\n",
        "feature_cols = [col+\"_Index\" for col in categorical_cols] + [col for col, dtype in df.dtypes if dtype in ['int', 'double'] and col != 'Churn']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Convert the label (Churn) to a numerical column\n",
        "label_indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
        "df = label_indexer.fit(df).transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Initialize the GBTClassifier with a higher maxBins\n",
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=1000, maxIter=100)\n",
        "\n",
        "# Train the model\n",
        "gbt_model = gbt.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = gbt_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model using AUC (Area Under the Curve)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test AUC: {auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcqBqQhFHcjk",
        "outputId": "f9d532a4-6917-4f81-c732-aefea0976c3c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUC: 0.8931034482758625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "AlCNKO7VJ4jx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "- [ChatGPT](https://chatgpt.com)\n",
        "- [Spark Documentation - GBTClassifier](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.GBTClassifier.html)\n"
      ],
      "metadata": {
        "id": "Pawg55mJLe8u"
      }
    }
  ]
}